{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6 - Research Paper Implementation\n",
    "#### Time Series Classification Using Multi-Channels Deep Convolutional Neural Networks \n",
    "#### Dataset: PAMPA2 dataset (http://archive.ics.uci.edu/ml/datasets/pamap2+physical+activity+monitoring)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Required Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split(data):\n",
    "    data_req = data[(data[1] == 3) | (data[1] == 4) | (data[1] == 12) | (data[1] == 13)]\n",
    "    data_channel = data_req[[1,38,39,40]]\n",
    "    data_time = data_req[[0]]\n",
    "    data_channel.columns = [0,1,2,3]\n",
    "    return data_time,data_channel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "directory = \"G:/DA - Hildeshim/DDA Lab/Exercise 6/Dastet/PAMAP2_Dataset/Protocol/\"\n",
    "for i in os.listdir(directory):\n",
    "    data = pd.read_csv(directory + i,header=None,sep=\" \")\n",
    "    data_time,data_channel = data_split(data)\n",
    "    data_channel = data_channel.reset_index()\n",
    "    data_channel = data_channel.drop(columns=\"index\")\n",
    "    data_channel.fillna(method=\"ffill\",inplace=True)\n",
    "    data_channel.to_csv(directory+i[:-4]+\".csv\",index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D data stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_stat = pd.DataFrame(columns=[\"start\",\"end\",\"Activity\"])\n",
    "new = 3\n",
    "start = 0\n",
    "cnt = 0\n",
    "end = 0\n",
    "for i,j in enumerate(data_activity[0]):\n",
    "    if new!=j:\n",
    "        end = i-1\n",
    "        data_stat.loc[cnt] = [start]+[end]+[data_activity[0][i-1]]\n",
    "        start = end\n",
    "        new = j\n",
    "        cnt+=1\n",
    "    \n",
    "data_stat.loc[4] = [start]+[data_activity.shape[0]]+[data_activity[0][i-1]]\n",
    "print(\"Activity details of Subject 1\")\n",
    "data_stat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMU Data Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=4, ncols=1,figsize=(30,45))\n",
    "ax[0].set_title(\"3D Acceleration Data from IMU Ankle of Subject 1\\n\",fontsize = 25)\n",
    "ax[0].plot(data_channel[3])\n",
    "ax[0].set_xlabel(\"Time\",fontsize=15)\n",
    "ax[0].set_ylabel(\"Data\",fontsize=15)\n",
    "ax[1].plot(data_channel[2])\n",
    "ax[1].set_xlabel(\"Time\",fontsize=15)\n",
    "ax[1].set_ylabel(\"Data\",fontsize=15)\n",
    "ax[2].plot(data_channel[1])\n",
    "ax[2].set_xlabel(\"Time\",fontsize=15)\n",
    "ax[2].set_ylabel(\"Data\",fontsize=15)\n",
    "ax[3].plot(data_channel[1])\n",
    "ax[3].plot(data_channel[2])\n",
    "ax[3].plot(data_channel[3])\n",
    "ax[3].set_xlabel(\"Time\",fontsize=15)\n",
    "ax[3].set_ylabel(\"3D Data\",fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalise Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise_data(data):\n",
    "    data[1] = (data[1] - np.mean(data[1])) / np.std(data[1])\n",
    "    data[2] = (data[2] - np.mean(data[2])) / np.std(data[2])\n",
    "    data[3] = (data[3] - np.mean(data[3])) / np.std(data[3])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot Activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activity(start,end,act):\n",
    "    y = np.zeros((4))\n",
    "    y[np.argmax(np.sum(act[start:end],axis=0))] = 1\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read IMU data from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    data = None\n",
    "    activity_data = None\n",
    "    for file in os.listdir(path):\n",
    "        data_in = pd.read_csv(path+file)\n",
    "        data_in.columns = [0,1,2,3]\n",
    "        data_channel = data_in[[1,2,3]]\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        data_channel = normalise_data(data_channel)\n",
    "        data_activity = data_in[[0]]\n",
    "        activity = np.array(pd.get_dummies(data_activity[0]))\n",
    "        channel_ip= []\n",
    "        channel_ac= []\n",
    "        for i in range(int((data_channel.shape[0] - 256) /128)):\n",
    "            if i == 0:\n",
    "                start = 0\n",
    "                end = 256\n",
    "            else:\n",
    "                start += 128\n",
    "                end +=128\n",
    "            channel_ip.append(data_channel[start:end])\n",
    "            channel_ac.append(get_activity(start,end,activity))\n",
    "        dat = np.stack(channel_ip)\n",
    "        act = np.stack(channel_ac)\n",
    "        if data is None:\n",
    "            data = dat\n",
    "            activity_data = act\n",
    "        else:\n",
    "            data = np.concatenate((data,dat),axis = 0)\n",
    "            activity_data = np.concatenate((activity_data,act),axis = 0)\n",
    "    return data,activity_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\raaghav\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "c:\\users\\raaghav\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "c:\\users\\raaghav\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "train_data_path = \"G:/DA - Hildeshim/DDA Lab/Exercise 6/Dastet/PAMAP2_Dataset/Train/\"\n",
    "test_data_path = \"G:/DA - Hildeshim/DDA Lab/Exercise 6/Dastet/PAMAP2_Dataset/Test/\"\n",
    "test, activity_test = read_data(test_data_path)\n",
    "train, activity_train = read_data(train_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv1d(x,weight,bias):\n",
    "    x = tf.nn.conv1d(x,weight,stride=1,padding=\"VALID\")\n",
    "    x = tf.nn.bias_add(x,bias)\n",
    "    x = tf.sigmoid(x)\n",
    "    return x\n",
    "\n",
    "def avgpool1d(x):\n",
    "    x = tf.layers.average_pooling1d(x,pool_size=2,strides=2,padding=\"VALID\")\n",
    "    return x\n",
    "\n",
    "def fc_layer(x,weight,bias):\n",
    "    x = tf.reshape(x,[-1,3*4*61])\n",
    "    x = tf.add(tf.matmul(x,weight),bias)\n",
    "    x = tf.sigmoid(x)\n",
    "    return x\n",
    "\n",
    "def out_layer(x,weight,bias):\n",
    "    x = tf.add(tf.matmul(x,weight),bias)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialise Variables and Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "input_len = 256\n",
    "n_channels = 3\n",
    "n_class = 4\n",
    "ip_ch_c1 = 3\n",
    "op_ch_c1 = 8 * n_channels\n",
    "op_ch_c2 = 4 * n_channels\n",
    "ip_ch_fc1 = 61 * op_ch_c2\n",
    "op_ch_fc1 = 4 * n_channels\n",
    "epochs = 1500\n",
    "batch_size = 1\n",
    "tf.reset_default_graph() \n",
    "\n",
    "x = tf.placeholder(tf.float32,[None,input_len,n_channels])\n",
    "y = tf.placeholder(tf.float32,[None,n_class])\n",
    "\n",
    "with tf.variable_scope(\"Weights\", reuse=tf.AUTO_REUSE):\n",
    "    w_conv1 = tf.get_variable('w_c1',shape=(5,ip_ch_c1,op_ch_c1),initializer=tf.random_normal_initializer)\n",
    "    w_conv2 = tf.get_variable('w_c2',shape=(5,op_ch_c1,op_ch_c2),initializer=tf.random_normal_initializer)\n",
    "    w_fc = tf.get_variable('w_fc',shape=(ip_ch_fc1,op_ch_fc1),initializer=tf.random_normal_initializer)\n",
    "    w_out = tf.get_variable('w_out',shape=(op_ch_fc1,n_class),initializer=tf.random_normal_initializer)\n",
    "    tf.summary.histogram('w1',w_conv1)\n",
    "    tf.summary.histogram('w2',w_conv2)\n",
    "    tf.summary.histogram('wfc',w_fc)\n",
    "    tf.summary.histogram('wout',w_out)\n",
    "    \n",
    "with tf.variable_scope(\"Bias\", reuse=tf.AUTO_REUSE):\n",
    "    b_conv1 = tf.get_variable('B_c1',shape=(op_ch_c1),initializer=tf.random_normal_initializer)\n",
    "    b_conv2 = tf.get_variable('B_c2',shape=(op_ch_c2),initializer=tf.random_normal_initializer)\n",
    "    b_fc = tf.get_variable('B_fc',shape=(op_ch_fc1),initializer=tf.random_normal_initializer)\n",
    "    b_out = tf.get_variable('B_out',shape=(n_class),initializer=tf.random_normal_initializer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MC_DCNN(x):\n",
    "    conv1 = conv1d(x,w_conv1,b_conv1)\n",
    "    avg_pool1 = avgpool1d(conv1)\n",
    "    conv2 = conv1d(avg_pool1,w_conv2,b_conv2)\n",
    "    avg_pool2 = avgpool1d(conv2)\n",
    "    fc = fc_layer(avg_pool2,w_fc,b_fc)\n",
    "    out = out_layer(fc,w_out,b_out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing optimizers and operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-10-a3e0d7e1d201>:3: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Accuracy_histogram:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MC_DCNN(x)\n",
    "\n",
    "predictions = tf.nn.softmax_cross_entropy_with_logits(logits=model,labels=y)\n",
    "tf.summary.histogram(\"predictions\", predictions)\n",
    "\n",
    "loss = tf.reduce_mean(predictions)\n",
    "tf.summary.scalar(\"Loss_Scalar\",loss)\n",
    "tf.summary.histogram(\"loss\",loss)\n",
    "\n",
    "optimizer_RMS = tf.train.RMSPropOptimizer(learning_rate=learning_rate,\n",
    "                                      decay=0.0005,\n",
    "                                      momentum=0.9,\n",
    "                                      epsilon=0.01).minimize(loss)\n",
    "\n",
    "optimizer_Adam = tf.train.AdamOptimizer(learning_rate=learning_rate,\n",
    "                                        epsilon=0.01).minimize(loss)\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(model, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "tf.summary.scalar(\"Accuracy\",accuracy)\n",
    "tf.summary.histogram(\"Accuracy_histogram\",accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1\tTraining loss: 3.434344\tTraining Accuracy: 0.30448893\n",
      "\t\tTesting loss: 3.0382938\tTesting Accuracy: 0.305949\n",
      "\n",
      "Epoch: 101\tTraining loss: 0.74434865\tTraining Accuracy: 0.7290427\n",
      "\t\tTesting loss: 0.80719507\tTesting Accuracy: 0.6827195\n",
      "\n",
      "Epoch: 201\tTraining loss: 0.59156466\tTraining Accuracy: 0.8053002\n",
      "\t\tTesting loss: 0.7271629\tTesting Accuracy: 0.73796034\n",
      "\n",
      "Epoch: 301\tTraining loss: 0.49842468\tTraining Accuracy: 0.841536\n",
      "\t\tTesting loss: 0.69391066\tTesting Accuracy: 0.75212467\n",
      "\n",
      "Epoch: 401\tTraining loss: 0.44140574\tTraining Accuracy: 0.86479175\n",
      "\t\tTesting loss: 0.67725956\tTesting Accuracy: 0.7790368\n",
      "\n",
      "Epoch: 501\tTraining loss: 0.3810678\tTraining Accuracy: 0.8912926\n",
      "\t\tTesting loss: 0.6868787\tTesting Accuracy: 0.77478755\n",
      "\n",
      "Epoch: 601\tTraining loss: 0.33676794\tTraining Accuracy: 0.91076255\n",
      "\t\tTesting loss: 0.6974701\tTesting Accuracy: 0.7818697\n",
      "\n",
      "Epoch: 701\tTraining loss: 0.29369763\tTraining Accuracy: 0.9302326\n",
      "\t\tTesting loss: 0.71878636\tTesting Accuracy: 0.7691218\n",
      "\n",
      "Epoch: 801\tTraining loss: 0.26333165\tTraining Accuracy: 0.9378042\n",
      "\t\tTesting loss: 0.7101521\tTesting Accuracy: 0.7733711\n",
      "\n",
      "Epoch: 901\tTraining loss: 0.24289902\tTraining Accuracy: 0.94213086\n",
      "\t\tTesting loss: 0.69491506\tTesting Accuracy: 0.786119\n",
      "\n",
      "Epoch: 1001\tTraining loss: 0.22934793\tTraining Accuracy: 0.9426717\n",
      "\t\tTesting loss: 0.6807064\tTesting Accuracy: 0.78470254\n",
      "\n",
      "Epoch: 1101\tTraining loss: 0.21968736\tTraining Accuracy: 0.94483507\n",
      "\t\tTesting loss: 0.67617077\tTesting Accuracy: 0.79036826\n",
      "\n",
      "Epoch: 1201\tTraining loss: 0.21082294\tTraining Accuracy: 0.94808006\n",
      "\t\tTesting loss: 0.6660875\tTesting Accuracy: 0.79320115\n",
      "\n",
      "Epoch: 1301\tTraining loss: 0.2059152\tTraining Accuracy: 0.94808006\n",
      "\t\tTesting loss: 0.66259366\tTesting Accuracy: 0.796034\n",
      "\n",
      "Epoch: 1401\tTraining loss: 0.20224671\tTraining Accuracy: 0.94808006\n",
      "\t\tTesting loss: 0.6603259\tTesting Accuracy: 0.8002833\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    train_writer = tf.summary.FileWriter(\"G:/DA - Hildeshim/DDA Lab/Exercise 6/log/1/train\",sess.graph)\n",
    "    test_writer = tf.summary.FileWriter(\"G:/DA - Hildeshim/DDA Lab/Exercise 6/log/1/test\",sess.graph)\n",
    "    merge = tf.summary.merge_all()    \n",
    "    sess.run(init)\n",
    "    counter = 0\n",
    "    for ep in range(epochs):\n",
    "        counter += 1\n",
    "#         for batch in range(len(train)//batch_size):\n",
    "#             batch_x = train[batch*batch_size:min((batch+1)*batch_size,len(train))]\n",
    "#             batch_y = activity_train[batch*batch_size:min((batch+1)*batch_size,len(activity_train))]\n",
    "        assert not np.any(np.isnan(train))\n",
    "        assert not np.any(np.isnan(activity_train))\n",
    "        feed = {x : train, y : activity_train}\n",
    "        summary, cost, _ , acc = sess.run([merge, loss, optimizer_RMS, accuracy], feed_dict = feed)\n",
    "        train_writer.add_summary(summary,counter)\n",
    "        if ep%100 == 1:\n",
    "            print(\"\\nEpoch: \"+str(ep)+\"\\tTraining loss: \"+str(cost)+\"\\tTraining Accuracy: \"+str(acc))\n",
    "            feed = {x : test, y : activity_test}\n",
    "            su, cost, acc = sess.run([merge, loss, accuracy], feed_dict = feed)\n",
    "            test_writer.add_summary(su,counter)\n",
    "            print(\"\\t\\tTesting loss: \"+str(cost)+\"\\tTesting Accuracy: \"+str(acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
